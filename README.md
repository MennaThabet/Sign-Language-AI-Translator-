# Sign-Language-AI-Translator-
The project is dedicated to translation of sign language into text to overcome the communication barriers between the world and deaf people.
A neural network takes an input passes it through multiple layers of hidden neurons, and outputs a prediction representing the combined input of all the neurons. After each cycle of training, an error metric is calculated based on the difference between prediction and target. The Python library “tesnsorflow.keras” was imported to provide optimized pre-trained model to be deployed in sign language application.
The imported data set was 87000 images, each one expresses a character in English sign language “ASL” as shown in fig (9) 
A 9:1 split ratio of data set in two folders- train and test- was done with 78300 images for training and 8700 images for testing. By this, the model can learn and memorize the information for future prediction. Because Ai models deal with small numbers, the images were resized and a scale of RGB was changed from 0:255 to 0:1. The Ai model was VGG16, as shown in fig (10), it was used because it is a very Deep Convolutional Network for Large-Scale Image Recognition. The model achieved 92.7% top-5 test accuracy in ImageNet, and it was trained for weeks on a dataset of over 14 million images belonging to 1000 classes. So, VGG16 was a best fit for reaching our purpose.
After inputting the images to VGG16, the neurons were activated with SoftMax activation function. The SoftMax activation function is typically placed in output layers of the network, the SoftMax neurons allow the prediction of the text as a mimicking for human brain. In Statistics 2.2.5, as we studied about probabilities, we related this concept with the function of SoftMax. It calculates the probabilities of distribution of the event over „n‟ different events. In general way of saying, this function will calculate the probabilities of each target class (the character) over all possible target classes (predicted characters), and the calculated probabilities will be helpful for determining the target class for the given inputs. Then, the loss function which is called “Categorical Cross-Entropy” used to calculate the rate of change of the error between y(text) and ŷ(predicted text) as shown in the equation: 
 Loss (ŷ, y) = - (y log(ŷ)) + (1-y) log (1-ŷ)
 
Methods of building the project:
First: - the TF model
The model depends on many blocks:
The first block (1): the keras library from the tensor flow platform was used and Numpy library as it has functions for working in matrices and keras contain two models and it was import from the functional model the main function for classification and detection which is vgg16 then import train-test split and the flatten layer was chosen as the dense layer as it convert 3d image to neuron then import matplotlib to plot the graphs.
The second block (2): it includes the output classes of the test and the place of the dataset and in the testing the result is ŷ then by comparing the y and ŷ it determines the accuracy. 
The third block (3): loading data from image, label, size, and index then the splitting between training image and testing image.
The fourth block (4): determining the batch number which was 128 and the epoch which is =5 and setting the graph data and calculation of the loss by the loss function loss (ŷ , y ) = - (y log(ŷ)) + (1-y) log (1-ŷ). 
The fifth block (5): setting the model then output the testing accuracy, time, and loss. 
Second: -the TF model was covert to Tflite by the official Tflite converter then the Tflite model was put in the react native as it has camera view that take photo and get it in the model.
